Exercise 9.1
Show that table-lookup TD(lambda) is a special case of general TD(lambda) as given by equations (9.5-9.7).

w_(t+1)=w_t+alpha*delta_t*z_t
delta_t=R_(t+1)+gamma*v(S_(t+1),w_t)-v(S_t,w_t)
z_t=gamma*lambda*z_(t-1)+grad_(w_t)v(S_t,w_t)

Exercise 9.2
State aggregation is a simple form of generalizing function approximation in which states are grouped together, with one table entry (value estimate) used for each group. Whenever a state in a group is encountered, the group's entry is used to determine the state's value, and when the state is updated, the group's entry is updated. Show that this kind of state aggregation is a special case of a gradient method such as (9.4).

Exercise 9.3
The equations given in this section are for the on-line version of gradient-descent TD(lambda). What are the equations for the off-line version? Give a complete description specifying the new weight vector at the end of an episode, w', in terms of the weight vector used during the episode, w. Start by modifying a forward-view equation for TD(lambda), such as (9.4).

Exercise 9.4
For off-line updating, show that equations (9.5-9.7) produce updates identical to (9.4).

Exercise 9.5
How could we reproduce the tabular case within the linear framework?



Exercise 9.6
How could we reproduce the state aggregation case (see Exercise 8.4) within the linear framework?
