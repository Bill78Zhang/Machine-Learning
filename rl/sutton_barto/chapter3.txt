Exercise 3.1
Devise three example tasks of your own that fit into the reinforcement learning framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.

Teaching a child to speak is an example of a reinforcement learning task where the states are the context of the conversation and the environment, the actions are choosing words to utter, and the rewards are a measure of how relevant the phrase is to the conversation (for example, how long the conversation continues after that point). Training a model for a self-driving car is another reinforcement learning task, where the states are the road, cars, pedestrians, and other aspects of the environment, the actions include braking, accelerating, and turning, and the car is rewarded for getting to its destination quickly but penalized for crashing. The classic scenario for reinforcement learning is playing games, where the state is usually a graphical representation of the game, the actions are constrained by the game controller, and the rewards are points accumulated as calculated by the game engine.

Exercise 3.2
Is the reinforcement learning framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?

There are some tasks in which the penalty for making a bad action in a particular state is so great that we would never want the learner to learn state-action values for those state-action pairs. One example of this is driving a robot off the edge of a cliff. In these situations, it is preferable to hard-code an algorithm that a learner will follow instead of allowing the learner to explore.

Exercise 3.3
Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out - say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in - say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?

The line between the agent and environment should be drawn so that the agent controller can consciously and directly control every aspect of the agent to a reasonably fine degree, but the action space should also be restricted so as to be tractable. Thus, defining actions as tire torques is too fine-grained because a driver cannot directly change the torques, and defining actions as muscle twitches leads to an intractable action space. These definitions depend on the exact form of the controller: if the controller is a humanoid robot, joint velocities may be a good action space, but if the controller is a navigation planner, defining actions as turn directions for reaching a destination is appropriate (given that some other system is in charge of steering the car).

Exercise 3.4
Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?

The return at each time t, where the final time step is T, is -gamma^(T-t) if pole-balancing is treated as an episodic task with discounting. If it is treated as a continuing task with discounting, the return is -gamma^(T_1-t)-gamma^(T_2-t)-gamma^(T_3-t)-..., where T_1,T_2,T_3,... are the times of the next failures occurring after time t.

Exercise 3.5
Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes - the successive runs through the maze - so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.1). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

Since a reward is only given for completing the maze and no penalties are applied, the robot does not learn to escape from the maze efficiently (in the least amount of time). In order to communicate this goal, we can add a penalty for every timestep in which the robot hasn't reached the end.

Exercise 3.6: Broken Vision System
Imagine that you are a vision system. When you are first turned on for the day, an image floods into your camera. You can see lots of things, but not all things. You can't see objects that are occluded, and of course you can't see objects that are behind you. After seeing that first scene, do you have access to the Markov state of the environment? Suppose your camera was broken that day and you received no images at all, all day. Would you have access to the Markov state then?

After seeing the first scene, the vision system does not have access to the Markov state of the environment. Since the camera cannot see objects that are occluded and has no idea that they are even present because it has no information from previous times, it does not have complete information about the environment that would allow it to predict the result of an action. This information is encoded in previous video frames which are unavailable to the camera. If the camera was broken all day, then from a certain point of view it has access to the Markov state. Since it is broken, the camera is in a constant state of not being able to see any images.

Exercise 3.7
Assuming a finite MDP with a finite number of reward values, write an equation for the transition probabilities and the expected rewards in terms of the joint conditional distribution in (3.5).

p(s'|a,s)=sum_r Pr{R_(t+1)=r,S_(t+1)=s'|S_t=s,A_t=a}
r(s',a,s)=sum_r r*Pr{R_(t+1)=r,S_(t+1)=s'|S_t=s,A_t=a}/p(s'|a,s)

Exercise 3.8
What is the Bellman equation for action values, that is, for q_pi? It must give the action value q_pi(s,a) in terms of the action values, q_pi(s',a'), of possible successors to the state-action pair (s,a). As a hint, the backup diagram corresponding to this equation is given in Figure 3.4b. Show the sequence of equations analogous to (3.10), but for action values.

q_pi(s,a)=sum_s' p(s'|a,s)(r(s,a,s')+gamma*sum_a' (pi(a'|s')q_pi(s',a')))

Exercise 3.9
The Bellman equation (3.10) must hold for each state for the value function v_pi shown in Figure 3.5b. As an example, show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4, and +0.7. (These numbers are accurate only to one decimal place.)

0.25*(0.9*2.3+0.9*0.4-0.9*0.4+0.9*0.7)=0.68

Exercise 3.10
In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.2), that adding a constant C to all the rewards adds a constant, K, to the values of all states, and thus does not affect the relative values of any states under any policies. What is K in terms of C and gamma?

G'_t=(R_(t+1)+C)+gamma*(R_(t+2)+C)+gamma^2*(R_(t+3)+C)+...=R_(t+1)+gamma*R_(t+2)+gamma^2*R_(t+3)+...+C+gamma*C+gamma^2*C+...=G_t+C/(1-gamma)
K=C/(1-gamma)

Exercise 3.11
Now consider adding a constant C to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.

Adding a constant C to all the rewards in an episodic task would not leave the task unchanged. If the values for all the states become positive, the agent will learn to avoid the end goal so that it can keep on accruing positive reward.

Exercise 3.12
The value of a state depends on the the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:
Give the equation corresponding to this intuition and diagram for the value at the root node, v_pi(s), in terms of the value at the expected leaf node, q_pi(s,a), given S_t = s. This expectation depends on the policy, pi. Then give a second equation in which the expected value is written out explicitly in terms of pi(a|s) such that no expected value notation appears in the equation.

v_pi(s)=E_a[q_pi(s,a)]
v_pi(s)=sum_a pi(a|s)*q_pi(s,a)

Exercise 3.13
The value of an action, q_pi(s,a), can be divided into two parts, the expected next reward, which does not depend on the policy pi, and the expected sum of the remaining rewards, which depends on the next state and the policy. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state-action pair) and branching to the possible next states:
Give the equation corresponding to this intuition and diagram for the action value, q_pi(s,a), in terms of the expected next reward, R_(t+1), and the expected next state value, v_pi(S_(t+1)), given that S_t=s and A_t=a. Then give a second equation, writing out the expected value explicitly in terms of p(s'|s,a) and r(s,a,s'), defined respectively by (3.6) and (3.7), such that no expected value
notation appears in the equation.

q_pi(s,a)=E_(S_(t+1))[R_(t+1)+gamma*v_pi(S_(t+1))]
q_pi(s,a)=sum_s' p(s'|a,s)*(r(s',a,s)+gamma*v_pi(s'))

Exercise 3.14
Draw or describe the optimal state-value function for the golf example.

The optimal state-value function for golf will be identical to the action-value function for driving shown in Figure 3.6b except on the green, where the value is -1 for the entire green. This is because v(s)=max_a q(s,a).

Exercise 3.15
Draw or describe the contours of the optimal action-value function for putting, q_*(s,putter), for the golf example.

The contours of the optimal action-value function for putting are identical to the state-value function for putting inside the green (shown in Figure 3.6a). Outside of the green, the contours with value -n are outside the contour with value -n+1 for the optimal action-value function for using the driver.

Exercise 3.16
Give the Bellman equation for q_* for the recycling robot.

q_*(h,s)=alpha*(r_search+gamma*max_a q_*(h,a))+(1-alpha)*(r_search+gamma*max_a q_*(l,a))
q_*(h,w)=r_wait+gamma*max_a q_*(h,a)
q_*(l,s)=(1-beta)*(-3+gamma*max_a q_*(h,a))+beta*(r_search+gamma*max_a q_*(l,a))
q_*(l,w)=r_wait+gamma*max_a q_*(l,a)
q_*(l,r)=gamma*max_a q_*(h,a)

Exercise 3.17
Figure 3.8 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.2) to express this value symbolically, and then to compute it to three decimal places.

G_t=10+10*0.9^5+10*0.9^10+...=10/(1-0.9^5)=24.419
